import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor

# Step 4: Data Structure and Content
df = pd.read_csv('train.csv')
num_rows, num_cols = df.shape
print(f"The dataset has {num_rows} rows and {num_cols} columns.")
sample_data = df.head(5)
print("Sample data:")
print(sample_data)
attributes = df.columns
print("Available attributes:")
print(attributes)
missing_values = df.isnull().sum()
print("Missing values:")
print(missing_values)

# Check if 'Item_Outlet_Sales' column exists
if 'Item_Outlet_Sales' not in df.columns:
    print("Error: 'Item_Outlet_Sales' column not found in the dataset. Please check the column name.")
    exit()

# Step 5: Exploratory Data Analysis
sns.histplot(df['Item_Outlet_Sales'], kde=True)
plt.xlabel('Item_Outlet_Sales')
plt.ylabel('Frequency')
plt.title('Distribution of Item_Outlet_Sales')
plt.show()

sns.scatterplot(x='Item_Weight', y='Item_Outlet_Sales', data=df)
plt.xlabel('Item Weight')
plt.ylabel('Item_Outlet_Sales')
plt.title('Item_Outlet_Sales vs. Item Weight')
plt.show()

sns.boxplot(x='Outlet_Type', y='Item_Outlet_Sales', data=df)
plt.xlabel('Outlet Type')
plt.ylabel('Item_Outlet_Sales')
plt.title('Item_Outlet_Sales by Outlet Type')
plt.show()

# Step 6: Univariate Analysis
summary_stats = df.describe()
print(summary_stats)

# Step 7: Bivariate Analysis
correlation_matrix = df.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

# Step 8: Missing Value Treatment
df = df.dropna()

# Step 9: Feature Engineering
df['Avg_Sales_Per_Store'] = df.groupby('Store')['Item_Outlet_Sales'].transform('mean')
df['Avg_Sales_Per_Category'] = df.groupby('Item_Category')['Item_Outlet_Sales'].transform('mean')

# Step 10: Encoding Categorical Variables
label_encoder = LabelEncoder()
df['Outlet_Type_Encoded'] = label_encoder.fit_transform(df['Outlet_Type'])

one_hot_encoder = OneHotEncoder(drop='first')
encoded_features = pd.DataFrame(one_hot_encoder.fit_transform(df[['Item_Fat_Content', 'Item_Type']]).toarray(),
                                columns=one_hot_encoder.get_feature_names(['Item_Fat_Content', 'Item_Type']))
df = pd.concat([df, encoded_features], axis=1)

# Step 11: Label Encoding (if applicable)
# Label encoding is not applicable in this case, as all categorical variables have been one-hot encoded

# Step 12: One Hot Encoding (if applicable)
# One-hot encoding has already been performed in Step 10

# Step 13: Preprocessing Data
scaler = StandardScaler()
scaled_features = scaler.fit_transform(df[['Item_Weight', 'Item_MRP']])
df[['Item_Weight', 'Item_MRP']] = scaled_features

# Step 14: Modeling
X = df.drop(['Item_Outlet_Sales'], axis=1)
y = df['Item_Outlet_Sales']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 15: Linear Regression
linear_reg = LinearRegression()
linear_reg.fit(X_train, y_train)
y_pred = linear_reg.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f"Linear Regression Mean Squared Error: {mse}")

# Step 16: Regularized Linear Regression
# Perform Lasso or Ridge regression if necessary

# Step 17: Random Forest
random_forest = RandomForestRegressor()
random_forest.fit(X_train, y_train)
y_pred = random_forest.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f"Random Forest Mean Squared Error: {mse}")

# Step 18: XGBoost
xgb = XGBRegressor()
xgb.fit(X_train, y_train)
y_pred = xgb.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f"XGBoost Mean Squared Error: {mse}")
